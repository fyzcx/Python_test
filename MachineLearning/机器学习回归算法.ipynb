{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-15T07:21:22.341636Z",
     "start_time": "2025-03-15T07:21:21.014792Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, LogisticRegression, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, classification_report, roc_auc_score\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.线性回归",
   "id": "80cf6def49ec4f4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 正规方程",
   "id": "d1bbe4a179f05d84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:40:01.304506Z",
     "start_time": "2025-03-15T07:40:01.291235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "线性回归直接预测房子价格\n",
    ":return: None\n",
    "\"\"\"\n",
    "# 获取数据\n",
    "fe_cal = fetch_california_housing(data_home='data')"
   ],
   "id": "d15e5b7b3a57f07",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "MedInc - 中位收入（Median Income）\n",
    "HouseAge - 房屋年龄（House Age）\n",
    "AveRooms - 平均房间数（Average Number of Rooms）\n",
    "AveBedrms - 平均卧室数（Average Number of Bedrooms）\n",
    "Population - 人口数量（Population）\n",
    "AveOccup - 平均居住人数（Average Occupancy）\n",
    "Latitude - 纬度（Latitude）\n",
    "Longitude - 经度（Longitude）"
   ],
   "id": "2ecb74b216c67b71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:40:03.276967Z",
     "start_time": "2025-03-15T07:40:03.264505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 分割数据集到训练集和测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(fe_cal.data, fe_cal.target, test_size=0.25, random_state=1)\n",
    "#\n",
    "print(x_train.shape)\n",
    "#\n",
    "# # 进行标准化处理(?) 目标值处理？\n",
    "# # 特征值和目标值是都必须进行标准化处理, 实例化两个标准化API\n",
    "std_x = StandardScaler()\n",
    "#\n",
    "x_train = std_x.fit_transform(x_train) #训练集标准化\n",
    "x_test = std_x.transform(x_test) #测试集标准化\n",
    "# print(y_train.shape)\n",
    "# 目标值进行了标准化，暂时没有对目标值进行标准化处理"
   ],
   "id": "4b0ebe0d7c3835ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15480, 8)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 未对目标值进行标准化处理的训练代码",
   "id": "5e59c55482eb1ed2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:40:38.790269Z",
     "start_time": "2025-03-15T07:40:38.778776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "# # estimator预测\n",
    "# # # 正规方程求解方式预测结果，正规方程进行线性回归\n",
    "lr = LinearRegression()\n",
    "# fit是耗时的\n",
    "lr.fit(x_train, y_train)\n",
    "#回归系数可以看特征与目标之间的相关性\n",
    "print('回归系数', lr.coef_)\n",
    "#\n",
    "y_predict = lr.predict(x_test)\n",
    "# 保存训练好的模型，模型中保存的是w的值，也保存了模型结构\n",
    "#保存模型放在fit之后即可\n",
    "joblib.dump(lr, \"./tmp/test.pkl\")\n",
    "print(\"正规方程测试集里面每个房子的预测价格：\", y_predict[0:10])\n",
    "#下面是求测试集的损失，用均方误差，公式是(y_test-y_predict)^2/n\n",
    "print(\"正规方程的均方误差：\", mean_squared_error(y_test, y_predict))"
   ],
   "id": "21f044830314dc1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回归系数 [ 0.83167028  0.12159502 -0.26758589  0.30983997 -0.00518054 -0.04040421\n",
      " -0.90736902 -0.88212727]\n",
      "正规方程测试集里面每个房子的预测价格： [2.12391852 0.93825754 2.7088455  1.70873764 2.82954754 3.50376456\n",
      " 3.0147162  1.62781292 1.74317518 2.01897806]\n",
      "正规方程的均方误差： 0.5356532845422556\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 对目标值进行标准化处理的训练代码",
   "id": "5ee17fefad938f6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:41:12.658643Z",
     "start_time": "2025-03-15T07:41:12.652740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "std_y = StandardScaler()\n",
    "# #\n",
    "# # #标签进行标准化\n",
    "# # 目标值是一维的，这里需要传进去2维的\n",
    "y_train = std_y.fit_transform(y_train.reshape(-1, 1))\n",
    "print(y_train.shape)"
   ],
   "id": "937724b3ae3b3066",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15480, 1)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:41:18.001703Z",
     "start_time": "2025-03-15T07:41:17.989188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr = LinearRegression()\n",
    "# fit是耗时的\n",
    "lr.fit(x_train, y_train)\n",
    "#回归系数可以看特征与目标之间的相关性\n",
    "print('回归系数', lr.coef_)\n",
    "#\n",
    "y_predict = lr.predict(x_test)\n",
    "# 保存训练好的模型，模型中保存的是w的值，也保存了模型结构\n",
    "# 预测测试集的房子价格，通过inverse得到真正的房子价格\n",
    "y_lr_predict = std_y.inverse_transform(y_predict)\n",
    "#保存模型放在fit之后即可\n",
    "os.unlink('./tmp/test.pkl') # 删除之前的模型文件\n",
    "joblib.dump(lr, \"./tmp/test.pkl\")\n",
    "print(\"正规方程测试集里面每个房子的预测价格：\", y_lr_predict[0:10])\n",
    "print(\"正规方程的均方误差：\", mean_squared_error(y_test, y_lr_predict))"
   ],
   "id": "12f011f3f3880b37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回归系数 [[ 0.71942632  0.10518431 -0.23147194  0.26802332 -0.00448136 -0.03495117\n",
      "  -0.7849086  -0.76307353]]\n",
      "正规方程测试集里面每个房子的预测价格： [[2.12391852]\n",
      " [0.93825754]\n",
      " [2.7088455 ]\n",
      " [1.70873764]\n",
      " [2.82954754]\n",
      " [3.50376456]\n",
      " [3.0147162 ]\n",
      " [1.62781292]\n",
      " [1.74317518]\n",
      " [2.01897806]]\n",
      "正规方程的均方误差： 0.5356532845422556\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 加载保存的模型",
   "id": "7ad52cf3d9c6bce7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:41:48.337618Z",
     "start_time": "2025-03-15T07:41:48.322320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#模拟上线时加载模型\n",
    "model = joblib.load(\"./tmp/test.pkl\")\n",
    "# # 因为目标值进行了标准化，一定要把预测后的值逆向转换回来\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "#\n",
    "# print(\"保存的模型预测的结果：\", y_predict[0:10])\n",
    "# print(\"正规方程的均方误差：\", mean_squared_error(y_test, y_predict))\n",
    "print(\"保存的y标准化后的模型预测的结果：\", std_y.inverse_transform(y_predict)[0:10])\n",
    "print(\"正规方程inverse后的均方误差：\", mean_squared_error(y_test,\n",
    "                                               std_y.inverse_transform(y_predict)))"
   ],
   "id": "b238dc438f6dce72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存的y标准化后的模型预测的结果： [[2.12391852]\n",
      " [0.93825754]\n",
      " [2.7088455 ]\n",
      " [1.70873764]\n",
      " [2.82954754]\n",
      " [3.50376456]\n",
      " [3.0147162 ]\n",
      " [1.62781292]\n",
      " [1.74317518]\n",
      " [2.01897806]]\n",
      "正规方程inverse后的均方误差： 0.5356532845422556\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. 随机梯度下降",
   "id": "f720c63627af6aa2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:43:06.052478Z",
     "start_time": "2025-03-15T07:43:05.831754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 梯度下降去进行房价预测,数据量大要用这个\n",
    "# learning_rate的不同方式，代表学习率变化的算法不一样,比如constant,invscaling,adaptive\n",
    "# 默认可以去调 eta0 = 0.008，会改变learning_rate的初始值\n",
    "# learning_rate='optimal',alpha是正则化力度，但是会影响学习率的值，由alpha来算学习率\n",
    "# penalty代表正则化，分为l1和l2\n",
    "# eta0=0.01, penalty='l2',max_iter=1000\n",
    "sgd = SGDRegressor(eta0=0.01,penalty='l2', max_iter=1000)\n",
    "# # 训练\n",
    "sgd.fit(x_train, y_train)\n",
    "#\n",
    "print('梯度下降的回归系数', sgd.coef_)\n",
    "#\n",
    "# 预测测试集的房子价格\n",
    "y_sgd_predict = std_y.inverse_transform(sgd.predict(x_test).reshape(-1, 1))\n",
    "y_predict = sgd.predict(x_test)\n",
    "print(\"梯度下降测试集里面每个房子的预测价格：\", y_sgd_predict)\n",
    "print(\"梯度下降的均方误差：\", mean_squared_error(y_test, y_predict))\n",
    "print(\"梯度下降的原始房价量纲均方误差：\", mean_squared_error(std_y.inverse_transform(y_test), y_sgd_predict))"
   ],
   "id": "4a115c19ef4f0cf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "梯度下降的回归系数 [ 0.71247388  0.09373967 -0.18748841  0.3485767  -0.00792621 -0.03112282\n",
      " -0.78929137 -0.74102292]\n",
      "梯度下降测试集里面每个房子的预测价格： [[2.11011488]\n",
      " [0.91077449]\n",
      " [2.61142493]\n",
      " ...\n",
      " [1.2841297 ]\n",
      " [2.74106016]\n",
      " [1.94615633]]\n",
      "梯度下降的均方误差： 4.791463156423583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[3.55  0.707 2.294 ... 0.588 2.108 0.875].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 18\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m梯度下降测试集里面每个房子的预测价格：\u001B[39m\u001B[38;5;124m\"\u001B[39m, y_sgd_predict)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m梯度下降的均方误差：\u001B[39m\u001B[38;5;124m\"\u001B[39m, mean_squared_error(y_test, y_predict))\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m梯度下降的原始房价量纲均方误差：\u001B[39m\u001B[38;5;124m\"\u001B[39m, mean_squared_error(\u001B[43mstd_y\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minverse_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m, y_sgd_predict))\n",
      "File \u001B[1;32mD:\\Program Files (x86)\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1106\u001B[0m, in \u001B[0;36mStandardScaler.inverse_transform\u001B[1;34m(self, X, copy)\u001B[0m\n\u001B[0;32m   1103\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m   1105\u001B[0m copy \u001B[38;5;241m=\u001B[39m copy \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy\n\u001B[1;32m-> 1106\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1107\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1108\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1109\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1110\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1112\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1113\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sparse\u001B[38;5;241m.\u001B[39missparse(X):\n\u001B[0;32m   1116\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwith_mean:\n",
      "File \u001B[1;32mD:\\Program Files (x86)\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1093\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m   1086\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1087\u001B[0m             msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1088\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected 2D array, got 1D array instead:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124marray=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00marray\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1089\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1090\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour data has a single feature or array.reshape(1, -1) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1091\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mif it contains a single sample.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1092\u001B[0m             )\n\u001B[1;32m-> 1093\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[0;32m   1095\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype_numeric \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(array\u001B[38;5;241m.\u001B[39mdtype, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkind\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUSV\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1096\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1097\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumeric\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not compatible with arrays of bytes/strings.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1098\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConvert your data to numeric values explicitly instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1099\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Expected 2D array, got 1D array instead:\narray=[3.55  0.707 2.294 ... 0.588 2.108 0.875].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4 岭回归",
   "id": "e017c2930d69a9fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:43:58.632413Z",
     "start_time": "2025-03-15T07:43:58.614487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # # 岭回归去进行房价预测\n",
    "#岭回归是对线性回归加入L2正则化，L2正则化是对系数的平方和进行惩罚\n",
    "#alpha就是补偿的系数\n",
    "#正规方程求解，加补偿就可以让正规方程可逆\n",
    "rd = Ridge(alpha=0.02)\n",
    "\n",
    "rd.fit(x_train, y_train)\n",
    "\n",
    "print(rd.coef_)\n",
    "#\n",
    "# # 预测测试集的房子价格\n",
    "print(rd.predict(x_test).shape)\n",
    "# y_rd_predict = std_y.inverse_transform(rd.predict(x_test))\n",
    "y_predict = rd.predict(x_test)\n",
    "# print(\"岭回归里面每个房子的预测价格：\", y_rd_predict)\n",
    "\n",
    "print(\"岭回归的均方误差：\", mean_squared_error(y_test, y_predict))\n",
    "# print(\"岭回归的均方误差：\", mean_squared_error(std_y.inverse_transform(y_test), y_rd_predict))"
   ],
   "id": "df0159d3cadaaa7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71942576  0.10518585 -0.23146889  0.26801931 -0.00448083 -0.03495127\n",
      " -0.78489401 -0.76305881]\n",
      "(5160,)\n",
      "岭回归的均方误差： 4.770500491883886\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5 lasso回归",
   "id": "bfa73f5cf582054a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:44:14.881074Z",
     "start_time": "2025-03-15T07:44:14.578933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # # Lasso回归去进行房价预测\n",
    "#alpha就是补偿的系数\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "ls = Lasso(alpha=0.001)\n",
    "\n",
    "ls.fit(x_train, y_train)\n",
    "\n",
    "print(ls.coef_)\n",
    "#\n",
    "# # 预测测试集的房子价格\n",
    "print(ls.predict(x_test).shape)\n",
    "print('-'*50)\n",
    "# y_ls_predict = std_y.inverse_transform(ls.predict(x_test).reshape(-1,1))\n",
    "y_predict = ls.predict(x_test)\n",
    "# print(\"Lasso回归里面每个房子的预测价格：\", y_rd_predict)\n",
    "#\n",
    "print(\"Lasso回归的均方误差：\", mean_squared_error(y_test, y_predict))\n",
    "# print(\"Lasso回归的均方误差：\", mean_squared_error(std_y.inverse_transform(y_test), y_ls_predict))"
   ],
   "id": "d5caf97d5d07e98c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15480, 8)\n",
      "(15480, 1)\n",
      "[ 0.71431792  0.10613811 -0.21758465  0.25415162 -0.00311065 -0.03403136\n",
      " -0.77399969 -0.75154125]\n",
      "(5160,)\n",
      "--------------------------------------------------\n",
      "Lasso回归的均方误差： 4.770944530791325\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:44:24.902919Z",
     "start_time": "2025-03-15T07:44:24.890798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 生成示例数据\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练 Elastic Net 模型\n",
    "model = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)  # alpha 是正则化强度，l1_ratio 是 alpha 参数\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 评估\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ],
   "id": "54dca33cd52790de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4638.838635649163\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5 逻辑回归",
   "id": "60b95038b0e3d0cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:51:03.330523Z",
     "start_time": "2025-03-15T07:51:03.271813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "逻辑回归做二分类进行癌症预测（根据细胞的属性特征）\n",
    ":return: NOne\n",
    "\"\"\"\n",
    "# 构造列标签名字\n",
    "column = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "          'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli',\n",
    "          'Mitoses', 'Class']\n",
    "\n",
    "# 读取数据\n",
    "# data = pd.read_csv(\n",
    "#     \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\",\n",
    "#     names=column)\n",
    "data = pd.read_csv(\n",
    "    \"./data/breast-cancer-wisconsin.csv\",\n",
    "    names=column)\n",
    "\n",
    "#当你读取数据时，看上去是数值的列，读进来是字符串，说明里边\n",
    "# 存在了非数值情况\n",
    "print(data.info())\n",
    "print('-'*50)\n",
    "data.describe(include='all')"
   ],
   "id": "289bec37d9dd668e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 699 entries, 0 to 698\n",
      "Data columns (total 11 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   Sample code number           699 non-null    int64 \n",
      " 1   Clump Thickness              699 non-null    int64 \n",
      " 2   Uniformity of Cell Size      699 non-null    int64 \n",
      " 3   Uniformity of Cell Shape     699 non-null    int64 \n",
      " 4   Marginal Adhesion            699 non-null    int64 \n",
      " 5   Single Epithelial Cell Size  699 non-null    int64 \n",
      " 6   Bare Nuclei                  699 non-null    object\n",
      " 7   Bland Chromatin              699 non-null    int64 \n",
      " 8   Normal Nucleoli              699 non-null    int64 \n",
      " 9   Mitoses                      699 non-null    int64 \n",
      " 10  Class                        699 non-null    int64 \n",
      "dtypes: int64(10), object(1)\n",
      "memory usage: 60.2+ KB\n",
      "None\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
       "count         6.990000e+02       699.000000               699.000000   \n",
       "unique                 NaN              NaN                      NaN   \n",
       "top                    NaN              NaN                      NaN   \n",
       "freq                   NaN              NaN                      NaN   \n",
       "mean          1.071704e+06         4.417740                 3.134478   \n",
       "std           6.170957e+05         2.815741                 3.051459   \n",
       "min           6.163400e+04         1.000000                 1.000000   \n",
       "25%           8.706885e+05         2.000000                 1.000000   \n",
       "50%           1.171710e+06         4.000000                 1.000000   \n",
       "75%           1.238298e+06         6.000000                 5.000000   \n",
       "max           1.345435e+07        10.000000                10.000000   \n",
       "\n",
       "        Uniformity of Cell Shape  Marginal Adhesion  \\\n",
       "count                 699.000000         699.000000   \n",
       "unique                       NaN                NaN   \n",
       "top                          NaN                NaN   \n",
       "freq                         NaN                NaN   \n",
       "mean                    3.207439           2.806867   \n",
       "std                     2.971913           2.855379   \n",
       "min                     1.000000           1.000000   \n",
       "25%                     1.000000           1.000000   \n",
       "50%                     1.000000           1.000000   \n",
       "75%                     5.000000           4.000000   \n",
       "max                    10.000000          10.000000   \n",
       "\n",
       "        Single Epithelial Cell Size Bare Nuclei  Bland Chromatin  \\\n",
       "count                    699.000000         699       699.000000   \n",
       "unique                          NaN          11              NaN   \n",
       "top                             NaN           1              NaN   \n",
       "freq                            NaN         402              NaN   \n",
       "mean                       3.216023         NaN         3.437768   \n",
       "std                        2.214300         NaN         2.438364   \n",
       "min                        1.000000         NaN         1.000000   \n",
       "25%                        2.000000         NaN         2.000000   \n",
       "50%                        2.000000         NaN         3.000000   \n",
       "75%                        4.000000         NaN         5.000000   \n",
       "max                       10.000000         NaN        10.000000   \n",
       "\n",
       "        Normal Nucleoli     Mitoses       Class  \n",
       "count        699.000000  699.000000  699.000000  \n",
       "unique              NaN         NaN         NaN  \n",
       "top                 NaN         NaN         NaN  \n",
       "freq                NaN         NaN         NaN  \n",
       "mean           2.866953    1.589413    2.689557  \n",
       "std            3.053634    1.715078    0.951273  \n",
       "min            1.000000    1.000000    2.000000  \n",
       "25%            1.000000    1.000000    2.000000  \n",
       "50%            1.000000    1.000000    2.000000  \n",
       "75%            4.000000    1.000000    4.000000  \n",
       "max           10.000000   10.000000    4.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample code number</th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.990000e+02</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.071704e+06</td>\n",
       "      <td>4.417740</td>\n",
       "      <td>3.134478</td>\n",
       "      <td>3.207439</td>\n",
       "      <td>2.806867</td>\n",
       "      <td>3.216023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.437768</td>\n",
       "      <td>2.866953</td>\n",
       "      <td>1.589413</td>\n",
       "      <td>2.689557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.170957e+05</td>\n",
       "      <td>2.815741</td>\n",
       "      <td>3.051459</td>\n",
       "      <td>2.971913</td>\n",
       "      <td>2.855379</td>\n",
       "      <td>2.214300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.438364</td>\n",
       "      <td>3.053634</td>\n",
       "      <td>1.715078</td>\n",
       "      <td>0.951273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.163400e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.706885e+05</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.171710e+06</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.238298e+06</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.345435e+07</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:51:14.797707Z",
     "start_time": "2025-03-15T07:51:14.785218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 缺失值进行处理\n",
    "data = data.replace(to_replace='?', value=np.nan)\n",
    "#直接删除，哪一行有空值，就删除对应的样本\n",
    "data = data.dropna()\n",
    "print('-' * 50)\n",
    "print(data.shape)\n",
    "print('-'*50)\n",
    "data.info()"
   ],
   "id": "4a0683eacf73e53a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "(683, 11)\n",
      "--------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 683 entries, 0 to 698\n",
      "Data columns (total 11 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   Sample code number           683 non-null    int64 \n",
      " 1   Clump Thickness              683 non-null    int64 \n",
      " 2   Uniformity of Cell Size      683 non-null    int64 \n",
      " 3   Uniformity of Cell Shape     683 non-null    int64 \n",
      " 4   Marginal Adhesion            683 non-null    int64 \n",
      " 5   Single Epithelial Cell Size  683 non-null    int64 \n",
      " 6   Bare Nuclei                  683 non-null    object\n",
      " 7   Bland Chromatin              683 non-null    int64 \n",
      " 8   Normal Nucleoli              683 non-null    int64 \n",
      " 9   Mitoses                      683 non-null    int64 \n",
      " 10  Class                        683 non-null    int64 \n",
      "dtypes: int64(10), object(1)\n",
      "memory usage: 64.0+ KB\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:51:24.233673Z",
     "start_time": "2025-03-15T07:51:24.228859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#把第6列的字符串转化为数字类型\n",
    "data[column[6]] = data[column[6]].astype('int16')"
   ],
   "id": "4b1b7a6b5c0cbf26",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:51:30.123818Z",
     "start_time": "2025-03-15T07:51:30.110371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 进行数据的分割,第零列是编号，不可以作为特征，把第1-9列作为特征，第10列作为标签\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[column[1:10]], data[column[10]], test_size=0.25,\n",
    "                                                    random_state=1)\n",
    "\n",
    "# 进行标准化处理\n",
    "std = StandardScaler()\n",
    "\n",
    "x_train = std.fit_transform(x_train) #训练集标准化\n",
    "x_test = std.transform(x_test) #测试集标准化\n",
    "x_train[0]"
   ],
   "id": "2621732efeb75b7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.21629973, -0.70863282, -0.75174943,  0.04301674, -0.55657068,\n",
       "       -0.71054972, -0.99312055, -0.62911518, -0.36280962])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:51:35.511781Z",
     "start_time": "2025-03-15T07:51:35.492737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# # 逻辑回归预测\n",
    "# C正则化力度,跟学习率有关\n",
    "# solver = 'liblinear'  solver是学习率优化算法，就是学习率会随着epoch的变化而变化\n",
    "#epoch就代表第几次迭代\n",
    "#max_iter 最大迭代次数\n",
    "lg = LogisticRegression(C=0.5, solver='lbfgs')\n",
    "#\n",
    "lg.fit(x_train, y_train)\n",
    "# 逻辑回归的权重参数，了解，没那么重要\n",
    "print(lg.coef_)\n",
    "\n",
    "y_predict = lg.predict(x_test)\n",
    "# print(y_predict) #预测的标签\n",
    "print(\"准确率：\", lg.score(x_test, y_test))\n",
    "print(y_test[0:5])\n",
    "print('-'*50)\n",
    "print(lg.predict_proba(x_test)[0:5])  #得出对应分类的概率\n"
   ],
   "id": "a09222d6f11ffa2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.11400191 0.25293086 0.78938469 0.60986034 0.0728013  1.10834397\n",
      "  0.7794668  0.64312128 0.67692658]]\n",
      "准确率： 0.9824561403508771\n",
      "444    2\n",
      "24     2\n",
      "195    2\n",
      "49     4\n",
      "375    2\n",
      "Name: Class, dtype: int64\n",
      "--------------------------------------------------\n",
      "[[0.94893919 0.05106081]\n",
      " [0.99494175 0.00505825]\n",
      " [0.98365149 0.01634851]\n",
      " [0.02707911 0.97292089]\n",
      " [0.99732446 0.00267554]]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:51:50.545557Z",
     "start_time": "2025-03-15T07:51:50.531558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 为什么还要看下召回率，labels和target_names对应\n",
    "# macro avg 平均值  weighted avg 加权平均值\n",
    "print(classification_report(y_test, y_predict, labels=[2, 4], target_names=[\"良性\", \"恶性\"]))\n",
    "#AUC计算要求是二分类，不需要是0和1\n",
    "print(\"AUC指标：\", roc_auc_score(y_test, y_predict))"
   ],
   "id": "57c93da1074bb3d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          良性       0.97      1.00      0.99       111\n",
      "          恶性       1.00      0.95      0.97        60\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.99      0.97      0.98       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n",
      "AUC指标： 0.975\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
